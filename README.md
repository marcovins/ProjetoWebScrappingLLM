
# Projeto de WebScraping com LLM's ğŸŒğŸ¤–

O seguinte projeto Ã© uma aplicaÃ§Ã£o para extraÃ§Ã£o e exibiÃ§Ã£o de informaÃ§Ãµes de pÃ¡ginas web, utilizando o modelo **SmartScraperGraph** e uma interface grÃ¡fica desenvolvida com **Tkinter**. A aplicaÃ§Ã£o combina scraping dinÃ¢mico com modelos LLM (Large Language Model) para gerar respostas estruturadas e metadados de pÃ¡ginas web.

> **Nota:** Este projeto exige uma configuraÃ§Ã£o local de modelo LLM e as variÃ¡veis de ambiente configuradas corretamente para acessar o modelo.

---

## Funcionalidades ğŸ› ï¸

- **Scraping DinÃ¢mico e EstÃ¡tico:** ğŸŒ
  - Lida com pÃ¡ginas dinÃ¢micas e estÃ¡ticas, usando Selenium quando necessÃ¡rio.
- **Interface GrÃ¡fica Intuitiva:** ğŸ–¥ï¸
  - Desenvolvida com **Tkinter**, facilita a interaÃ§Ã£o para inserir URLs e exibir resultados.
- **Fallback DinÃ¢mico:** ğŸ”„
  - Usa `HandlerDinamic` para pÃ¡ginas com conteÃºdo carregado dinamicamente.
- **ValidaÃ§Ã£o Estruturada:** âœ…
  - Implementa **Pydantic** para validar e organizar os dados extraÃ­dos.
- **Resultados Detalhados:** ğŸ“Š
  - Exibe dados extraÃ­dos, como descriÃ§Ãµes, tags, confianÃ§a do modelo e tempo de processamento.

---

## Requisitos âš™ï¸

### DependÃªncias de Sistema ğŸ’»

- **Python 3.6+** 
- **Google Chrome** e **ChromeDriver** (para scraping dinÃ¢mico com Selenium).

### DependÃªncias de Python ğŸ“¦

- `requests`
- `pydantic`
- `tkinter` (disponÃ­vel por padrÃ£o em distribuiÃ§Ãµes do Python)
- `selenium`
- `scrapegraphai`
- `beautifulsoup4`
- `python-dotenv`
  
Instale todas as dependÃªncias com:

```bash
pip install -r requirements.txt
```

> **Nota:** Configure uma LLM local. As variÃ¡veis de ambiente `MODEL_URL` e `PROMPT` devem apontar para o endpoint configurado.

---

## Como Usar ğŸ“

1. **Configurar as variÃ¡veis de ambiente:** âš™ï¸
   - Crie um arquivo `.env` e configure:
     ```env
     MODEL_URL=http://localhost:5000
     PROMPT="Extraia informaÃ§Ãµes estruturadas desta pÃ¡gina"
     ```

2. **Iniciar a aplicaÃ§Ã£o:** ğŸš€
   - Execute o script principal:
     ```bash
     python src/main.py
     ```

3. **Interagir com a interface:** ğŸ®
   - Insira a URL no campo de texto.
   - Clique em "Executar Scraper".
   - Visualize os resultados detalhados na Ã¡rea de saÃ­da.

---

## Exemplo de SaÃ­da ğŸ’¡

Um exemplo de resposta estruturada:

```json
{
    "descricao": "Texto extraÃ­do da pÃ¡gina web...",
    "tag": "Categoria da pÃ¡gina",
    "tokens_usados": 100,
    "tempo_processamento": 2.3,
    "confianca": 0.95,
    "metadados": {
        "source_url": "http://example.com",
        "author": "Autor do conteÃºdo"
    }
}
```

---

## Estrutura do Projeto ğŸ—‚ï¸

```plaintext
rsc/
â”œâ”€â”€chromedriver-win32/ # Arquivos necessÃ¡rios para utilizar o chromedriver
â”‚   â””â”€â”€chromedriver.exe 
â”‚   â””â”€â”€LICENSE,chromedriver
â”‚   â””â”€â”€THIRD_PARTY_NOTICES.chromedriver

src/
â”œâ”€â”€Scrapping
â”‚   â””â”€â”€ Scrapper.py           # Script principal com interface grÃ¡fica
â”‚   â””â”€â”€ DinamicScrapper.py    # Scraper dinÃ¢mico com Selenium
â”‚   â””â”€â”€main.py # Arquivo principal de execuÃ§Ã£o
â”œâ”€â”€ Schemas/
â”‚   â””â”€â”€ ResponseSchema.py # Esquema de validaÃ§Ã£o dos dados extraÃ­dos
test/
â”œâ”€â”€ Scrapper_tests.py # Arquivo de testes
```

---

## Contribuindo ğŸ¤

ContribuiÃ§Ãµes sÃ£o bem-vindas! Siga os passos abaixo:

1. FaÃ§a um fork do repositÃ³rio.
2. Crie uma branch para suas alteraÃ§Ãµes (`git checkout -b feature/descricao`).
3. FaÃ§a commit das mudanÃ§as (`git commit -m "DescriÃ§Ã£o das mudanÃ§as"`).
4. Envie a branch para o repositÃ³rio remoto (`git push origin feature/descricao`).
5. Abra um Pull Request.

---

## Contato ğŸ“¬

Se vocÃª tiver dÃºvidas, sugestÃµes ou encontrar problemas, abra uma **issue** ou envie um **pull request**.
