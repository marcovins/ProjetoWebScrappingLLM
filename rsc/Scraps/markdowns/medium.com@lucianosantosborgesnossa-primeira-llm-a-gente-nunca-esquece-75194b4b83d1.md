# Nossa primeira LLM a gente nunca esquece!<br><br>[![Luciano Santos Borges](https://miro.medium.com/v2/resize:fill:88:88/0*6xvQJ_JrasbdRWp4.)](/@lucianosantosborges?source=post_page---byline--75194b4b83d1--------------------------------)<br><br>[Luciano Santos Borges](/@lucianosantosborges?source=post_page---byline--75194b4b83d1--------------------------------)<br><br>¬∑<br><br>[Follow](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c05424295c9&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40lucianosantosborges%2Fnossa-primeira-llm-a-gente-nunca-esquece-75194b4b83d1&user=Luciano+Santos+Borges&userId=8c05424295c9&source=post_page-8c05424295c9--byline--75194b4b83d1---------------------post_header-----------)<br><br>11 min read<br><br>¬∑<br><br>Jul 16, 2024<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F75194b4b83d1&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40lucianosantosborges%2Fnossa-primeira-llm-a-gente-nunca-esquece-75194b4b83d1&user=Luciano+Santos+Borges&userId=8c05424295c9&source=---header_actions--75194b4b83d1---------------------clap_footer-----------)<br><br>9<br><br>3<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75194b4b83d1&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40lucianosantosborges%2Fnossa-primeira-llm-a-gente-nunca-esquece-75194b4b83d1&source=---header_actions--75194b4b83d1---------------------bookmark_footer-----------)<br><br>Share<br><br>**TL;DR:** Este artigo documenta o processo de cria√ß√£o de uma _Large Language Model_(LLM) do zero. Antes de entrar na teoria e no c√≥digo, preciso desabafar sobre o desafio que foi treinar minha primeira IA. Escolher a √°rea foi o primeiro passo, no caso, o tema foi **moda** , seguido pela cria√ß√£o de uma base de dados em formato de **perguntas e respostas** , come√ßando com 1000 pares e aumentando para 1500. Tentei usar o modelo **T5** , mas os resultados foram insatisfat√≥rios devido ao baixo volume de dados em portugu√™s. Passei ent√£o para o **GPT-2** (distilgpt2) e, mesmo assim, os primeiros resultados n√£o foram bons. Traduzi os dados para o ingl√™s e aumentei a quantidade, o que finalmente fez o modelo funcionar melhor, embora as respostas ainda n√£o estejam perfeitas üò∞.<br><br>![](https://miro.medium.com/v2/resize:fit:700/1*GXjOgLi30vcKyMg0HhOxbA.jpeg)<br><br>Imagem gerada pelo DALL-E<br><br>A t√£o falada Intelig√™ncia Artificial (IA) busca capacitar as m√°quinas a imitar fun√ß√µes cognitivas humanas. Uma inova√ß√£o importante nessa √°rea s√£o os **modelos de transformadores** , utilizados em modelos de linguagem de grande escala (LLMs), como o GPT da OpenAI. No frigir dos ovos üç≥, os LLMs s√£o projetados para entender e gerar texto natural, mas √© importante notar que eles n√£o ‚Äúentendem‚Äù no sentido humano. Eles identificam e replicam padr√µes baseados nos dados com os quais foram treinados. Esses modelos utilizam uma estrutura chamada ‚Äú** _transformers_** ‚Äù ü§ñ para melhorar a efici√™ncia do treinamento, permitindo que eles funcionem prevendo a pr√≥xima palavra em um texto com grande precis√£o.<br><br>A arquitetura _Transformer_ pode ser dividida em tr√™s tipos principais: **Encoder** , **Decoder** e modelos **Encoder-Decoder.** Modelos baseados em _Encoder_ (BERT, RoBERTa) s√£o usados para tarefas de compreens√£o de texto, enquanto modelos baseados em _Decoder_ (GPT) s√£o usados para gera√ß√£o de texto. Modelos _Encoder-Decoder_ (T5) combinam ambas as funcionalidades.<br><br>O pr√©-processamento de dados para modelos de linguagem de grande escala (LLMs) envolve v√°rias etapas detalhadas. Inicialmente, os dados s√£o divididos em pequenas unidades chamadas tokens, que podem ser palavras, subpalavras, n√∫meros ou s√≠mbolos. Em seguida, os dados s√£o limpos para remover erros, conte√∫do ofensivo ou spam. O texto √© ent√£o normalizado, o que pode incluir a convers√£o de todas as letras para min√∫sculas, remo√ß√£o de _stopwords_ , e aplica√ß√£o de t√©cnicas de _stemming_ ou _lematiza√ß√£o_. Por fim, esses tokens s√£o transformados em n√∫meros atrav√©s de t√©cnicas como embedding, para que o modelo possa process√°-los eficientemente. **O objetivo deste artigo √© mostrar exatamente essas etapas na pr√°tica, permitindo que voc√™ crie seu pr√≥prio modelo.**<br><br>> Nem todos os LLMs realizam todas essas etapas, especialmente a remo√ß√£o de stopwords, stemming ou lematiza√ß√£o, que podem ser menos comuns em alguns modelos modernos.<br><br>Mas preciso confessar, eu menti ü§• para voc√™! Em vez de ensinar a criar um **LLM (_Large Language Model_)**, na verdade vamos construir um **SLM (_Small Language Model_)**. Embora sejam menores, os SLMs oferecem vantagens significativas em rela√ß√£o aos LLMs. Eles s√£o mais econ√¥micos em termos de treinamento e uso, respondem mais rapidamente devido ao menor tempo de processamento, consomem menos energia e s√£o mais f√°ceis de implementar em dispositivos com recursos limitados, como smartphones e gadgets inteligentes. Essas caracter√≠sticas tornam os SLMs ideais para aplica√ß√µes que exigem respostas r√°pidas e efici√™ncia, como chatbots e assistentes virtuais, embora possam n√£o alcan√ßar o mesmo n√≠vel de desempenho ou precis√£o em tarefas mais complexas comparado aos LLMs.<br><br>![](https://miro.medium.com/v2/resize:fit:700/0*wk1RKhWbopxcMeHw.jpg)<br><br>> Para treinar um LLM, √© necess√°rio equipamento especializado, como GPUs ou TPUs de alto desempenho, para processar grandes volumes de dados e realizar c√°lculos complexos. Neste exemplo, utilizarei o Google Colab, que oferece acesso gratuito a GPUs e TPUs , possibilitando um treinamento sem a necessidade de investir em hardware caro.<br><br>![](https://miro.medium.com/v2/resize:fit:700/0*MqonOTxcjDX-qkf9)<br><br>> O c√≥digo explicado abaixo pode ser encontrado no reposit√≥rio: <br><br>**Passo 1 ‚Äî Crie um Google Colab e habilite a T4 GPU, para isto basta ir no menu _Edit > Notebook settings_.**<br><br>![](https://miro.medium.com/v2/resize:fit:700/1*oMWSq-Mb0dlE3jASuwhkfA.png)<br><br>**Passo 2 ‚Äî Instala√ß√£o das bibliotecas necess√°rias**<br><br>```<br>!pip install transformers[torch] datasets torch<br>```<br><br>A biblioteca **_transformers_** da oferece modelos de linguagem pr√©-treinados como GPT-2, BERT e T5 para tarefas como gera√ß√£o de texto, resposta a perguntas e tradu√ß√£o. √â f√°cil de usar e permite treinar ou ajustar modelos para necessidades espec√≠ficas. √â uma ferramenta essencial para aplicar intelig√™ncia artificial em linguagem natural. A biblioteca **_torch_** √© usada para computa√ß√£o e treinamento eficiente, e a biblioteca **_datasets_** √© usada para manipular nossos dados de treino.<br><br>**Passo 3 ‚Äî Importa√ß√£o das bibliotecas**<br><br>```<br>import torchfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArgumentsfrom datasets import Datasetimport jsonimport pandas as pd<br>```<br><br>Neste passo, importamos as bibliotecas essenciais para manipula√ß√£o de dados e aprendizado profundo. Utilizamos o `torch` para opera√ß√µes de tensor e computa√ß√£o em GPU; a biblioteca `transformers` para carregar e treinar o modelo GPT-2, incluindo `GPT2Tokenizer`, `GPT2LMHeadModel`, `Trainer` e `TrainingArguments`; a `datasets` do Hugging Face para preparar e manipular conjuntos de dados; a biblioteca padr√£o `json` para leitura de arquivos JSON; e o `pandas` para transformar dados JSON em DataFrames para an√°lise e manipula√ß√£o eficientes. Essas bibliotecas s√£o fundamentais para realizar tarefas de processamento de linguagem natural (NLP) de maneira eficaz.<br><br>**Passo 4 ‚Äî Carregar dados do arquivo JSON**<br><br>```<br>def load_data(file_path): with open(file_path, 'r') as file: data = json.load(file) return pd.DataFrame(data)file_path = '/content/dataset.json'df = load_data(file_path)<br>```<br><br>O objetivo desta etapa √© carregar os dados contidos em um arquivo JSON e convert√™-los em um DataFrame do Pandas. Um DataFrame √© uma estrutura de dados bidimensional que facilita a manipula√ß√£o e an√°lise dos dados. Voc√™ pode acessar o dataset que criei atrav√©s do seguinte link: .<br><br>> A cria√ß√£o deste dataset foi um verdadeiro parto. Utilizei uma s√©rie de prompts para gerar JSONs contendo perguntas e respostas sobre uma ampla gama de t√≥picos relacionados √† moda. Esses t√≥picos inclu√≠am moda feminina, masculina e infantil, acess√≥rios, vestu√°rio para ocasi√µes espec√≠ficas, cal√ßados, estilos sazonais e recomenda√ß√µes para diferentes tipos de corpo. Para garantir a diversidade das informa√ß√µes, utilizei o **ChatGPT** , **Gemini** e **Claude**. Ap√≥s concluir esse processo, percebi ü´£ que poderia ter usado uma crewAI para gerar esses dados (üí° fica a ideia para um pr√≥ximo artigo). Vale informar que desenvolvi um script para identificar üîé e eliminar Ô∏èÔ∏èüóëÔ∏è perguntas duplicadas no JSON. √â muito importante n√£o ter perguntas repetidas.<br><br>**Passo 5 ‚Äî Inicializar tokenizador e preparar dados**<br><br>```<br>tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')tokenizer.pad_token = tokenizer.eos_tokendf['text'] = df['question'] + " " + df['answer']def tokenize_function(examples): return tokenizer(examples, padding="max_length", truncation=True, max_length=512)tokenized_datasets = df['text'].apply(lambda x: tokenize_function(x))dataset = Dataset.from_pandas(df)def tokenize_dataset(dataset): return dataset.map(lambda x: tokenizer(x['text'], padding="max_length", truncation=True, max_length=512), batched=True, remove_columns=["text", "question", "answer"])tokenized_dataset = tokenize_dataset(dataset)<br>```<br><br>Neste passo, inicializamos o tokenizador `GPT2Tokenizer` do modelo `distilgpt2` e configuramos o token de padding para ser o mesmo que o token de fim de sequ√™ncia (EOS). Concatenamos perguntas e respostas em uma nova coluna `text` no DataFrame e definimos uma fun√ß√£o de tokeniza√ß√£o que aplica padding e truncamento at√© um comprimento m√°ximo de 512 tokens. Aplicamos esta fun√ß√£o de tokeniza√ß√£o a cada texto no DataFrame, convertendo-o em tokens. Em seguida, transformamos o DataFrame em um `Dataset` do Hugging Face, facilitando o processamento eficiente e removendo colunas originais ap√≥s a tokeniza√ß√£o para otimiza√ß√£o.<br><br>> O token de padding √© utilizado para garantir que todas as sequ√™ncias de entrada em um lote (batch) de dados tenham o mesmo comprimento. Isso √© necess√°rio porque os modelos de aprendizado profundo, como os transformadores, requerem que as entradas tenham dimens√µes consistentes para processamento eficiente em paralelo.<br><br>**Passo 6 ‚Äî Adicionar labels e dividir dataset**<br><br>```<br>tokenized_dataset = tokenized_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True)train_test_split = tokenized_dataset.train_test_split(test_size=0.15)train_dataset = train_test_split['train']test_dataset = train_test_split['test']<br>```<br><br>Este passo √© crucial para preparar os dados para o treinamento do modelo. Aqui, os dados tokenizados recebem labels iguais aos `input_ids` para calcular a perda corretamente, aplicando uma transforma√ß√£o que cria uma nova coluna `labels` nos exemplos do dataset. Em seguida, o dataset √© dividido em conjuntos de treino (85%) e valida√ß√£o (15%) usando a fun√ß√£o `train_test_split`[_esses valores podem ser ajustados_], garantindo uma avalia√ß√£o justa do modelo. Essa divis√£o permite treinar o modelo em um subconjunto de dados e valid√°-lo em outro, evitando overfitting e assegurando que o modelo seja avaliado com dados n√£o vistos durante o treinamento. Esses processos s√£o essenciais para a prepara√ß√£o adequada dos dados e o treinamento eficiente do modelo.<br><br>**Passo 7‚ÄîFun√ß√£o de agrupamento de dados**<br><br>```<br>def data_collator(features): batch = {} batch['input_ids'] = torch.tensor([f['input_ids'] for f in features], dtype=torch.long) batch['attention_mask'] = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long) batch['labels'] = torch.tensor([f['labels'] for f in features], dtype=torch.long) return batch<br>```<br><br>A fun√ß√£o `data_collator` organiza dados tokenizados em batches para treinamento de modelos de linguagem, convertendo listas de `input_ids`, `attention_mask` e `labels` em tensores PyTorch. Essa fun√ß√£o √© essencial para efici√™ncia computacional, consist√™ncia no treinamento e manuseio adequado de padding, preparando os dados de forma estruturada para serem usados pelo `Trainer` do Hugging Face. Ela cria um dicion√°rio contendo esses tensores agrupados, garantindo que cada batch tenha a mesma estrutura, facilitando opera√ß√µes vetorizadas e c√°lculos de perda durante o treinamento.<br><br>> Tensores PyTorch s√£o estruturas de dados fundamentais similares aos arrays multidimensionais mas com a vantagem adicional de serem otimizados para opera√ß√µes de alto desempenho em GPUs (unidades de processamento gr√°fico).<br><br>**Passo 8 ‚Äî Inicializar modelo e argumentos de treinamento**<br><br>```<br>model = GPT2LMHeadModel.from_pretrained('distilgpt2')training_args = TrainingArguments( output_dir='./results', eval_strategy="epoch", learning_rate=3e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=5, weight_decay=0.01, logging_dir='./logs', logging_steps=10,)<br>```<br><br>Neste passo, inicializamos o modelo GPT-2 com `GPT2LMHeadModel.from_pretrained('distilgpt2')`, que carrega uma vers√£o otimizada e menor do GPT-2. Em seguida, definimos os argumentos de treinamento utilizando `TrainingArguments`, especificando par√¢metros como diret√≥rio de sa√≠da, estrat√©gia de avalia√ß√£o, taxa de aprendizado, tamanhos de batch, n√∫mero de √©pocas, decaimento de peso, diret√≥rio de logs e frequ√™ncia de registro. Esses argumentos configuram o `Trainer`, que gerencia o treinamento e a avalia√ß√£o do modelo com base nos datasets fornecidos e na fun√ß√£o de agrupamento de dados (`data_collator`), garantindo um processo de treinamento eficiente e monitorado.<br><br>**Passo 9 ‚Äî Configurar o trainer e iniciar o treinamento**<br><br>```<br>trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset, data_collator=data_collator)trainer.train()<br>```<br><br>Este passo envolve a configura√ß√£o do objeto `Trainer` da biblioteca `transformers`, que gerencia o treinamento do modelo GPT-2. Configuramos o `Trainer` com o modelo, par√¢metros de treinamento, conjuntos de dados de treino e avalia√ß√£o, e uma fun√ß√£o para agrupar dados em batches. Conforme visto no passo 8, os par√¢metros de treinamento incluem a taxa de aprendizado, tamanho dos batches, n√∫mero de √©pocas, e diret√≥rios para salvar resultados e logs. Ap√≥s a configura√ß√£o, o treinamento √© iniciado com `trainer.train()`, que ajusta o modelo com base nos dados e par√¢metros fornecidos, realizando _forward pass_ , c√°lculo da perda, _backward pass_ e atualiza√ß√£o dos par√¢metros do modelo ao longo de v√°rias √©pocas.<br><br>> O **forward pass** √© a etapa em que os dados de entrada s√£o passados pela rede neural, camada por camada, at√© que uma previs√£o (ou sa√≠da) seja gerada.<br>> <br>> O **c√°lculo da perda** √© a etapa em que a previs√£o do modelo √© comparada com a verdade real (ou r√≥tulo) usando uma fun√ß√£o de perda (ou fun√ß√£o de custo). A fun√ß√£o de perda mede o qu√£o boa ou ruim √© a previs√£o do modelo. O objetivo do treinamento √© minimizar essa perda.<br>> <br>> O **backward pass** (ou backpropagation) √© a etapa em que os gradientes da fun√ß√£o de perda em rela√ß√£o aos par√¢metros do modelo s√£o calculados, e os par√¢metros s√£o ajustados para minimizar a perda.<br>> <br>> Na **Atualiza√ß√£o de Par√¢metros** , usamos os gradientes calculados para atualizar os par√¢metros da rede. Normalmente, isso √© feito usando um otimizador que aplica uma regra de atualiza√ß√£o aos par√¢metros baseada nos gradientes e na taxa de aprendizado.<br><br>√â bom lembrar que esse processo pode levar algum tempo, dependendo do tamanho do dataset e da complexidade do modelo. Abaixo podemos ver o output gerado no Google Colab.<br><br>![](https://miro.medium.com/v2/resize:fit:539/1*q5gZs5-elotBHE9ZJvfGWQ.png)<br><br>```<br>TrainOutput(global_step=800, **training_loss=0.2628884120285511** , metrics={'train_runtime': 692.0873, 'train_samples_per_second': 9.211, 'train_steps_per_second': 1.156, 'total_flos': 832883392512000.0, 'train_loss': 0.2628884120285511, 'epoch': 5.0})<br>```<br><br>Traduzindo essas informa√ß√µes, vemos que o treinamento levou quase 12 minutos e durante esse tempo, o modelo processou dados rapidamente, realizando pouco mais de 1 passo de treinamento por segundo. O modelo foi treinado em 5 ciclos (epochs) completos atrav√©s dos dados, resultando em uma perda final de **0,2629** , que √© uma medida de erro, indicando que o modelo conseguiu aprender bem com os dados de treinamento. Em resumo, o treinamento foi eficiente e o modelo parece ter aprendido de forma satisfat√≥ria.<br><br>**Passo 10 ‚Äî Salvar o modelo e tokenizador treinados**<br><br>```<br>model.save_pretrained("./gpt2-chatbot")tokenizer.save_pretrained("./gpt2-chatbot")<br>```<br><br>Ap√≥s o treinamento do modelo, √© crucial salvar tanto o modelo quanto o tokenizador para reutiliza√ß√£o futura. Isso √© feito utilizando os m√©todos `save_pretrained` do `GPT2LMHeadModel` e `GPT2Tokenizer`, que armazenam os pesos treinados, configura√ß√µes, e vocabul√°rio em um diret√≥rio especificado, como `./gpt2-chatbot`. Salvar esses componentes permite carreg√°-los posteriormente com `from_pretrained`, evitando a necessidade de retraining, facilitando o compartilhamento, backup e versionamento do modelo, garantindo efici√™ncia e consist√™ncia nas infer√™ncias futuras.<br><br>**Passo 11 ‚Äî Carregar modelo e tokenizador treinados**<br><br>```<br>model = GPT2LMHeadModel.from_pretrained("./gpt2-chatbot")tokenizer = GPT2Tokenizer.from_pretrained("./gpt2-chatbot")<br>```<br><br>Este passo envolve carregar o modelo e o tokenizador treinados usando a fun√ß√£o `from_pretrained()`, que permite reutilizar os pesos ajustados do modelo GPT-2 e as configura√ß√µes do tokenizador sem precisar retrainar. Isso √© essencial para aplica√ß√µes pr√°ticas, como responder perguntas dos usu√°rios em um aplicativo web. O c√≥digo carrega o modelo e o tokenizador do diret√≥rio `./gpt2-chatbot`, e inclui uma fun√ß√£o `gerar_resposta` (passo 12) que tokeniza a entrada do usu√°rio, gera uma resposta com o modelo, e decodifica a sa√≠da para texto. Este processo garante infer√™ncias r√°pidas e consistentes com o treinamento.<br><br>**Passo 12 ‚Äî Fun√ß√£o para gerar resposta**<br><br>```<br>def gerar_resposta(model, tokenizer, input_text, max_length=50, num_return_sequences=1): inputs = tokenizer.encode(input_text, return_tensors='pt') attention_mask = [1] * len(inputs[0]) outputs = model.generate(inputs, attention_mask=torch.tensor([attention_mask]), max_length=max_length, num_return_sequences=num_return_sequences, pad_token_id=tokenizer.eos_token_id) generated_text = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs] return generated_text<br>```<br><br>A fun√ß√£o `gerar_resposta` utiliza um modelo de linguagem treinado e um tokenizador para gerar respostas a partir de um texto de entrada. Primeiro, o texto de entrada √© tokenizado e uma m√°scara de aten√ß√£o √© criada. Em seguida, o modelo gera a resposta usando os tokens de entrada, com par√¢metros ajust√°veis como o comprimento m√°ximo da resposta (`max_length`) e o n√∫mero de sequ√™ncias retornadas (`num_return_sequences`). As respostas geradas s√£o ent√£o decodificadas de volta para texto leg√≠vel, removendo tokens especiais. A fun√ß√£o √© flex√≠vel e permite ajustes adicionais conforme necess√°rio para diferentes tarefas de gera√ß√£o de texto.<br><br>> M√°scara de aten√ß√£o √© uma ferramenta usada em modelos de linguagem, especialmente em arquiteturas de transformadores, como GPT-2, para controlar quais tokens (palavras ou sub-palavras) em uma sequ√™ncia de entrada devem ser considerados (ou ‚Äúatendidos‚Äù) pelo modelo em diferentes etapas de processamento.<br><br>**Passo 13 ‚Äî Exemplo de uso da fun√ß√£o de gera√ß√£o de resposta**<br><br>```<br>input_text = "What are the best colors to use in winter?"resposta = gerar_resposta(model, tokenizer, input_text)print(f"{resposta}")<br>```<br><br>No exemplo acima, a fun√ß√£o `gerar_resposta` √© usada para responder √† pergunta **"What are the best colors to use in winter?"** , demonstrando a flexibilidade para ajustar par√¢metros como o comprimento m√°ximo da resposta e o n√∫mero de sequ√™ncias retornadas. Essa fun√ß√£o √© √∫til para criar chatbots e sistemas de resposta autom√°tica que necessitam de gera√ß√£o de texto baseado em modelos de linguagem.<br><br>Abaixo podemos ver a resposta, podemos ver que o resultado n√£o est√° perfeito, pois, existem informa√ß√µes repetidas.<br><br>![](https://miro.medium.com/v2/resize:fit:700/1*NP5aUFWDZBiL78ub6RSjAQ.png)<br><br>Ufa ü•µ, que jornada, hein? Como dizem em minha terra: ‚ÄúRapadura √© doce, mas n√£o √© mole n√£o!‚Äù Em uma √©poca em que todos querem usar a IA, percebo que, para fazer um uso mais aprofundado, √© preciso dedica√ß√£o e esfor√ßo. N√£o vou mentir: no in√≠cio, os resultados podem n√£o ser muito impressionantes. Mas n√£o desanime! Com um pouco de perseveran√ßa e mais dados, voc√™ ver√° o modelo melhorando aos poucos.<br><br>E a√≠, o que achou dessa aventura? Espero que tenha te dado aquela inspira√ß√£o pra mergulhar nesse mundo fascinante. Se curtiu, que tal deixar um joinha? E n√£o seja t√≠mido, conta pra mim nos coment√°rios o que passou pela sua cabe√ßa. Sua opini√£o √© super importante pra mim!<br><br>[Gpt2 Chatbot](/tag/gpt2-chatbot?source=post_page-----75194b4b83d1--------------------------------)<br><br>[Small Language Model](/tag/small-language-model?source=post_page-----75194b4b83d1--------------------------------)<br><br>[Large Language Models](/tag/large-language-models?source=post_page-----75194b4b83d1--------------------------------)<br><br>[Artificial Intelligence](/tag/artificial-intelligence?source=post_page-----75194b4b83d1--------------------------------)<br><br>[Machine Learning](/tag/machine-learning?source=post_page-----75194b4b83d1--------------------------------)<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F75194b4b83d1&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40lucianosantosborges%2Fnossa-primeira-llm-a-gente-nunca-esquece-75194b4b83d1&user=Luciano+Santos+Borges&userId=8c05424295c9&source=---footer_actions--75194b4b83d1---------------------clap_footer-----------)<br><br>9<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F75194b4b83d1&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40lucianosantosborges%2Fnossa-primeira-llm-a-gente-nunca-esquece-75194b4b83d1&user=Luciano+Santos+Borges&userId=8c05424295c9&source=---footer_actions--75194b4b83d1---------------------clap_footer-----------)<br><br>9<br><br>3<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75194b4b83d1&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40lucianosantosborges%2Fnossa-primeira-llm-a-gente-nunca-esquece-75194b4b83d1&source=---footer_actions--75194b4b83d1---------------------bookmark_footer-----------)<br><br>[![Luciano Santos Borges](https://miro.medium.com/v2/resize:fill:96:96/0*6xvQJ_JrasbdRWp4.)](/@lucianosantosborges?source=post_page---post_author_info--75194b4b83d1--------------------------------)<br><br>[![Luciano Santos Borges](https://miro.medium.com/v2/resize:fill:128:128/0*6xvQJ_JrasbdRWp4.)](/@lucianosantosborges?source=post_page---post_author_info--75194b4b83d1--------------------------------)<br><br>Follow<br><br>## [Written by Luciano Santos Borges](/@lucianosantosborges?source=post_page---post_author_info--75194b4b83d1--------------------------------)<br><br>[110 Followers](/@lucianosantosborges/followers?source=post_page---post_author_info--75194b4b83d1--------------------------------)<br><br>¬∑[52 Following](/@lucianosantosborges/following?source=post_page---post_author_info--75194b4b83d1--------------------------------)<br><br>Product Owner<br><br>Follow<br><br>## Responses (3)<br><br>[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--75194b4b83d1--------------------------------)<br><br>[What are your thoughts?](/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40lucianosantosborges%2Fnossa-primeira-llm-a-gente-nunca-esquece-75194b4b83d1&source=---post_responses--75194b4b83d1---------------------respond_sidebar-----------)<br><br>Cancel<br><br>Respond<br><br>Also publish to my profile<br><br>![Rodrigo Gon√ßalves](https://miro.medium.com/v2/resize:fill:32:32/0*zqmA_nu0b5g2M932.)<br><br>[Rodrigo Gon√ßalves](/@ro-goncalves?source=post_page---post_responses--75194b4b83d1----0----------------------------)<br><br>[5 months ago](/@ro-goncalves/muito-bom-os-seus-artigos-leio-todos-eles-me-inspiram-a-fazer-o-mesmo-238a93f1a9a9?source=post_page---post_responses--75194b4b83d1----0----------------------------)<br><br>```<br><br><br>Muito bom os seus artigos, leio todos, eles me inspiram a fazer o mesmo. Uma d√∫vida, como eu treino modelos que completa uma frase? Por exemplo: batatinha quando nasce [MASK], nesse caso o modelo ir√° preencher e a palavra que deve ser colocada em [MASK]<br><br><br>```<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F238a93f1a9a9&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40ro-goncalves%2Fmuito-bom-os-seus-artigos-leio-todos-eles-me-inspiram-a-fazer-o-mesmo-238a93f1a9a9&user=Rodrigo+Gon%C3%A7alves&userId=e8db21f40d66&source=---post_responses--238a93f1a9a9----0-----------------respond_sidebar-----------)<br><br>1<br><br>1 reply<br><br>Reply<br><br>![Airton Carneiro](https://miro.medium.com/v2/resize:fill:32:32/0*gXuQg1gd_-8-YKDH.)<br><br>[Airton Carneiro](/@airtoncarneiro?source=post_page---post_responses--75194b4b83d1----1----------------------------)<br><br>[5 months ago](/@airtoncarneiro/muito-legal-tinha-curiosidade-de-ver-um-processo-de-treinamento-de-xlm-5bc5e95a3bad?source=post_page---post_responses--75194b4b83d1----1----------------------------)<br><br>```<br><br><br>Muito legal! Tinha curiosidade de "ver" um processo de treinamento de xLM.<br><br><br>```<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F5bc5e95a3bad&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40airtoncarneiro%2Fmuito-legal-tinha-curiosidade-de-ver-um-processo-de-treinamento-de-xlm-5bc5e95a3bad&user=Airton+Carneiro&userId=be31f7f0ef9e&source=---post_responses--5bc5e95a3bad----1-----------------respond_sidebar-----------)<br><br>1<br><br>Reply<br><br>![Airton Carneiro](https://miro.medium.com/v2/resize:fill:32:32/0*gXuQg1gd_-8-YKDH.)<br><br>[Airton Carneiro](/@airtoncarneiro?source=post_page---post_responses--75194b4b83d1----2----------------------------)<br><br>[5 months ago](/@airtoncarneiro/muito-legal-tinha-curiosidade-de-ver-um-processo-de-treinamento-de-xlm-5f0c47e8b715?source=post_page---post_responses--75194b4b83d1----2----------------------------)<br><br>```<br><br><br>Muito legal! Tinha curiosidade de "ver" um processo de treinamento de xLM.<br><br><br>```<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F5f0c47e8b715&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40airtoncarneiro%2Fmuito-legal-tinha-curiosidade-de-ver-um-processo-de-treinamento-de-xlm-5f0c47e8b715&user=Airton+Carneiro&userId=be31f7f0ef9e&source=---post_responses--5f0c47e8b715----2-----------------respond_sidebar-----------)<br><br>1<br><br>Reply<br><br>## More from Luciano Santos Borges<br><br>![#1 crewAI: prepare-se para a Rebeli√£o das M√°quinas!](https://miro.medium.com/v2/resize:fit:679/1*KBPkv9qoIYuln3l3bQc4pA.png)<br><br>[![Luciano Santos Borges](https://miro.medium.com/v2/resize:fill:20:20/0*6xvQJ_JrasbdRWp4.)](/@lucianosantosborges?source=post_page---author_recirc--75194b4b83d1----0---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>[Luciano Santos Borges](/@lucianosantosborges?source=post_page---author_recirc--75194b4b83d1----0---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>## [#1 crewAI: prepare-se para a Rebeli√£o das M√°quinas!TL;DR: Nesta s√©rie de artigos, vou apresentar um exemplo mais complexo, onde utilizarei todos os recursos que aprendi no curso de Jo√£o‚Ä¶](/@lucianosantosborges/1-crewai-prepare-se-para-a-rebeli√£o-das-m√°quinas-9da80bdf5c9d?source=post_page---author_recirc--75194b4b83d1----0---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>May 20<br><br>[21](/@lucianosantosborges/1-crewai-prepare-se-para-a-rebeli√£o-das-m√°quinas-9da80bdf5c9d?source=post_page---author_recirc--75194b4b83d1----0---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9da80bdf5c9d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40lucianosantosborges%2F1-crewai-prepare-se-para-a-rebeli%25C3%25A3o-das-m%25C3%25A1quinas-9da80bdf5c9d&source=---author_recirc--75194b4b83d1----0-----------------bookmark_preview----b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>![Business Model Canvas com ChatGPT](https://miro.medium.com/v2/resize:fit:679/1*oRB2pQhwoN61JhEcDEzH1A.png)<br><br>[![Luciano Santos Borges](https://miro.medium.com/v2/resize:fill:20:20/0*6xvQJ_JrasbdRWp4.)](/@lucianosantosborges?source=post_page---author_recirc--75194b4b83d1----1---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>[Luciano Santos Borges](/@lucianosantosborges?source=post_page---author_recirc--75194b4b83d1----1---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>## [Business Model Canvas com ChatGPTTL;DR: Durante a elabora√ß√£o da s√©rie de artigos que demonstram como o ChatGPT pode ser um valioso aliado na jornada de idea√ß√£o e‚Ä¶](/@lucianosantosborges/business-model-canvas-com-chatgpt-8c3d4cb829ae?source=post_page---author_recirc--75194b4b83d1----1---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>May 4<br><br>[6](/@lucianosantosborges/business-model-canvas-com-chatgpt-8c3d4cb829ae?source=post_page---author_recirc--75194b4b83d1----1---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c3d4cb829ae&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40lucianosantosborges%2Fbusiness-model-canvas-com-chatgpt-8c3d4cb829ae&source=---author_recirc--75194b4b83d1----1-----------------bookmark_preview----b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>![#2 crewAI: prepare-se para a Rebeli√£o das M√°quinas!](https://miro.medium.com/v2/resize:fit:679/1*dH6OHIXfftbR37fanoqjug.png)<br><br>[![Luciano Santos Borges](https://miro.medium.com/v2/resize:fill:20:20/0*6xvQJ_JrasbdRWp4.)](/@lucianosantosborges?source=post_page---author_recirc--75194b4b83d1----2---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>[Luciano Santos Borges](/@lucianosantosborges?source=post_page---author_recirc--75194b4b83d1----2---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>## [#2 crewAI: prepare-se para a Rebeli√£o das M√°quinas!TL;DR: Neste segundo artigo da s√©rie, vamos aprofundar nosso projeto no crewAI, focado na cria√ß√£o de uma equipe respons√°vel por elaborar um‚Ä¶](/@lucianosantosborges/2-crewai-prepare-se-para-a-rebeli√£o-das-m√°quinas-9934664374aa?source=post_page---author_recirc--75194b4b83d1----2---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>May 23<br><br>[15](/@lucianosantosborges/2-crewai-prepare-se-para-a-rebeli√£o-das-m√°quinas-9934664374aa?source=post_page---author_recirc--75194b4b83d1----2---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9934664374aa&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40lucianosantosborges%2F2-crewai-prepare-se-para-a-rebeli%25C3%25A3o-das-m%25C3%25A1quinas-9934664374aa&source=---author_recirc--75194b4b83d1----2-----------------bookmark_preview----b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>![Da Ideia ao MVP com o Aux√≠lio do ChatGPT‚Ää‚Äî‚ÄäUm Estudo de Caso‚Ää‚Äî‚ÄäParte 6](https://miro.medium.com/v2/resize:fit:679/1*4BDkSpj4s3yAAJD3SfOe5g.png)<br><br>[![Luciano Santos Borges](https://miro.medium.com/v2/resize:fill:20:20/0*6xvQJ_JrasbdRWp4.)](/@lucianosantosborges?source=post_page---author_recirc--75194b4b83d1----3---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>[Luciano Santos Borges](/@lucianosantosborges?source=post_page---author_recirc--75194b4b83d1----3---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>## [Da Ideia ao MVP com o Aux√≠lio do ChatGPT ‚Äî Um Estudo de Caso ‚Äî Parte 6TL;DR: Nesta sexta parte da nossa s√©rie veremos como criar as Hist√≥rias de Usu√°rio (User Stories) com o aux√≠lio do ChatGPT.](/@lucianosantosborges/da-ideia-ao-mvp-com-o-aux√≠lio-do-chatgpt-um-estudo-de-caso-parte-6-75987761c863?source=post_page---author_recirc--75194b4b83d1----3---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>Apr 14<br><br>[5](/@lucianosantosborges/da-ideia-ao-mvp-com-o-aux√≠lio-do-chatgpt-um-estudo-de-caso-parte-6-75987761c863?source=post_page---author_recirc--75194b4b83d1----3---------------------b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75987761c863&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40lucianosantosborges%2Fda-ideia-ao-mvp-com-o-aux%25C3%25ADlio-do-chatgpt-um-estudo-de-caso-parte-6-75987761c863&source=---author_recirc--75194b4b83d1----3-----------------bookmark_preview----b36b44c0_6c6f_4e7d_90dd_196e5281f628-------)<br><br>[See all from Luciano Santos Borges](/@lucianosantosborges?source=post_page---author_recirc--75194b4b83d1--------------------------------)<br><br>## Recommended from Medium<br><br>![Agentic AI: Building Autonomous Systems from Scratch](https://miro.medium.com/v2/resize:fit:679/1*84o9zsmdc68VqqZ9pu6Zjg.png)<br><br>[![Towards Data Science](https://miro.medium.com/v2/resize:fill:20:20/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://medium.com/towards-data-science?source=post_page---read_next_recirc--75194b4b83d1----0---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>In<br><br>[Towards Data Science](https://medium.com/towards-data-science?source=post_page---read_next_recirc--75194b4b83d1----0---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>by<br><br>[Lu√≠s Roque](/@luisroque?source=post_page---read_next_recirc--75194b4b83d1----0---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>## [Agentic AI: Building Autonomous Systems from ScratchA Step-by-Step Guide to Creating Multi-Agent Frameworks in the Age of Generative AI](/towards-data-science/agentic-ai-building-autonomous-systems-from-scratch-8f80b07229ea?source=post_page---read_next_recirc--75194b4b83d1----0---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>5d ago<br><br>[4755](/towards-data-science/agentic-ai-building-autonomous-systems-from-scratch-8f80b07229ea?source=post_page---read_next_recirc--75194b4b83d1----0---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f80b07229ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fagentic-ai-building-autonomous-systems-from-scratch-8f80b07229ea&source=---read_next_recirc--75194b4b83d1----0-----------------bookmark_preview----6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>![Why Llama 3.3 70B Is So Much Better Than GPT-4o And Claude 3.5 Sonnet‚Ää‚Äî‚ÄäHere The Result](https://miro.medium.com/v2/resize:fit:679/1*NQXNKn6aDqDN_4R4O-HA4g.png)<br><br>[![Towards AI](https://miro.medium.com/v2/resize:fill:20:20/1*JyIThO-cLjlChQLb6kSlVQ.png)](https://medium.com/towards-artificial-intelligence?source=post_page---read_next_recirc--75194b4b83d1----1---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>In<br><br>[Towards AI](https://medium.com/towards-artificial-intelligence?source=post_page---read_next_recirc--75194b4b83d1----1---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>by<br><br>[Gao Dalie (È´òÈÅîÁÉà)](/@GaoDalie_AI?source=post_page---read_next_recirc--75194b4b83d1----1---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>## [Why Llama 3.3 70B Is So Much Better Than GPT-4o And Claude 3.5 Sonnet ‚Äî Here The ResultAI news in the past 7 days has been insane, with so much happening in the world of AI.](/towards-artificial-intelligence/why-llama-3-3-70b-is-so-much-better-than-gpt-4o-and-claude-3-5-sonnet-here-the-result-f1436e6b992a?source=post_page---read_next_recirc--75194b4b83d1----1---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>4d ago<br><br>[3224](/towards-artificial-intelligence/why-llama-3-3-70b-is-so-much-better-than-gpt-4o-and-claude-3-5-sonnet-here-the-result-f1436e6b992a?source=post_page---read_next_recirc--75194b4b83d1----1---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1436e6b992a&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fwhy-llama-3-3-70b-is-so-much-better-than-gpt-4o-and-claude-3-5-sonnet-here-the-result-f1436e6b992a&source=---read_next_recirc--75194b4b83d1----1-----------------bookmark_preview----6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>## Lists<br><br>[![](https://miro.medium.com/v2/resize:fill:48:48/1*1SA9firObg-qNWD96hYD8w.png)![](https://miro.medium.com/v2/da:true/resize:fill:48:48/0*59TWXvZgazD4Q6BW)![](https://miro.medium.com/v2/resize:fill:48:48/1*w-hiT0-jA0PdwvalWi7xgw.png)Natural Language Processing1862 stories¬∑1489 saves](/@AMGAS14/list/natural-language-processing-0a856388a93a?source=post_page---read_next_recirc--75194b4b83d1--------------------------------)<br><br>[![](https://miro.medium.com/v2/resize:fill:48:48/0*r4yjMpEmqzHCUvWC.jpg)![](https://miro.medium.com/v2/resize:fill:48:48/1*bv2KUVNLi2sFNjBTdoBmWw.png)![](https://miro.medium.com/v2/resize:fill:48:48/0*zsngbTOmFCy6sUCx.jpeg)Predictive Modeling w/ Python20 stories¬∑1724 saves](/@ben.putney/list/predictive-modeling-w-python-e3668ea008e1?source=post_page---read_next_recirc--75194b4b83d1--------------------------------)<br><br>[![](https://miro.medium.com/v2/resize:fill:48:48/1*era76EGCwdY2gWSFKutuSw.jpeg)![](https://miro.medium.com/v2/resize:fill:48:48/1*AiTJDz5wwQFiUCf_SrBOQA.jpeg)![A phone with a tweet on it describing a deepfake video of the Ukrainian president, with a labeled fake image in the background](https://miro.medium.com/v2/resize:fill:48:48/1*zjPggFS8yoRtFbAP9R_3lw.jpeg)AI Regulation6 stories¬∑650 saves](/@MediumStaff/list/ai-regulation-dfa78dfd2438?source=post_page---read_next_recirc--75194b4b83d1--------------------------------)<br><br>[![Principal Component Analysis for ML](https://miro.medium.com/v2/resize:fill:48:48/1*swd_PY6vTCyPnsgBYoFZfA.png)![Time Series Analysis](https://miro.medium.com/v2/resize:fill:48:48/1*8sSAHftNwd_RNJ3k4VA0pA.png)![deep learning cheatsheet for beginner](https://miro.medium.com/v2/resize:fill:48:48/1*uNyD4yNMH-DnOel1wzxOOA.png)Practical Guides to Machine Learning10 stories¬∑2095 saves](https://destingong.medium.com/list/practical-guides-to-machine-learning-a877c2a39884?source=post_page---read_next_recirc--75194b4b83d1--------------------------------)<br><br>![A list of git commits](https://miro.medium.com/v2/resize:fit:679/0*1VLsvF2ztRxsOhSf)<br><br>[![Andrew Zuo](https://miro.medium.com/v2/resize:fill:20:20/1*FZEG_DxaZ4g-w10VST7WGg.jpeg)](/@impure?source=post_page---read_next_recirc--75194b4b83d1----0---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>[Andrew Zuo](/@impure?source=post_page---read_next_recirc--75194b4b83d1----0---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>## [AI Is Killing CodingThere‚Äôs a new IDE out called Cursor. Although as I said before:](/@impure/ai-is-killing-coding-d3afd96cf356?source=post_page---read_next_recirc--75194b4b83d1----0---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>Dec 9<br><br>[1.4K53](/@impure/ai-is-killing-coding-d3afd96cf356?source=post_page---read_next_recirc--75194b4b83d1----0---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd3afd96cf356&operation=register&redirect=https%3A%2F%2Fandrewzuo.com%2Fai-is-killing-coding-d3afd96cf356&source=---read_next_recirc--75194b4b83d1----0-----------------bookmark_preview----6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>![Someone Has Made an Uncensored Version of QwQ-32B-Preview, And It Is Awesome](https://miro.medium.com/v2/resize:fit:679/0*ELpFba3FPHAZKbf4.png)<br><br>[![Sebastian Petrus](https://miro.medium.com/v2/resize:fill:20:20/1*jRkLFadAFjkcyEN5bCnVZA.png)](/@sebastian-petrus?source=post_page---read_next_recirc--75194b4b83d1----1---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>[Sebastian Petrus](/@sebastian-petrus?source=post_page---read_next_recirc--75194b4b83d1----1---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>## [Someone Has Made an Uncensored Version of QwQ-32B-Preview, And It Is AwesomeQwQ-32B-Preview is Awesome, and more powerful that GPT-4o-mini, but It Is Censored. Not Any More.](/@sebastian-petrus/someone-has-mad-an-uncensored-version-of-qwq-32b-preview-and-it-is-awesome-f64769a60d03?source=post_page---read_next_recirc--75194b4b83d1----1---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>Dec 3<br><br>[1912](/@sebastian-petrus/someone-has-mad-an-uncensored-version-of-qwq-32b-preview-and-it-is-awesome-f64769a60d03?source=post_page---read_next_recirc--75194b4b83d1----1---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff64769a60d03&operation=register&redirect=https%3A%2F%2Fsebastian-petrus.medium.com%2Fsomeone-has-mad-an-uncensored-version-of-qwq-32b-preview-and-it-is-awesome-f64769a60d03&source=---read_next_recirc--75194b4b83d1----1-----------------bookmark_preview----6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>![I used OpenAI‚Äôs o1 model to develop a trading strategy. It is DESTROYING the market](https://miro.medium.com/v2/resize:fit:679/1*MuD60qiJYZ1GJSSraELZpg.png)<br><br>[![DataDrivenInvestor](https://miro.medium.com/v2/resize:fill:20:20/1*2mBCfRUpdSYRuf9EKnhTDQ.png)](https://medium.com/datadriveninvestor?source=post_page---read_next_recirc--75194b4b83d1----2---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>In<br><br>[DataDrivenInvestor](https://medium.com/datadriveninvestor?source=post_page---read_next_recirc--75194b4b83d1----2---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>by<br><br>[Austin Starks](/@austin-starks?source=post_page---read_next_recirc--75194b4b83d1----2---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>## [I used OpenAI‚Äôs o1 model to develop a trading strategy. It is DESTROYING the marketIt literally took one try. I was shocked.](/datadriveninvestor/i-used-openais-o1-model-to-develop-a-trading-strategy-it-is-destroying-the-market-576a6039e8fa?source=post_page---read_next_recirc--75194b4b83d1----2---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>Sep 15<br><br>[7.2K177](/datadriveninvestor/i-used-openais-o1-model-to-develop-a-trading-strategy-it-is-destroying-the-market-576a6039e8fa?source=post_page---read_next_recirc--75194b4b83d1----2---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F576a6039e8fa&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Fi-used-openais-o1-model-to-develop-a-trading-strategy-it-is-destroying-the-market-576a6039e8fa&source=---read_next_recirc--75194b4b83d1----2-----------------bookmark_preview----6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>![Understanding Graph-based RAG Systems: A Deep Dive into GraphRAG and LightRAG](https://miro.medium.com/v2/resize:fit:679/1*C1m8cKyDE1pe48Xqg1Z1GA.png)<br><br>[![Generative AI](https://miro.medium.com/v2/resize:fill:20:20/1*M4RBhIRaSSZB7lXfrGlatA.png)](https://medium.com/generative-ai?source=post_page---read_next_recirc--75194b4b83d1----3---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>In<br><br>[Generative AI](https://medium.com/generative-ai?source=post_page---read_next_recirc--75194b4b83d1----3---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>by<br><br>[Satyabrata Dash](/@dashingSat?source=post_page---read_next_recirc--75194b4b83d1----3---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>## [Understanding Graph-based RAG Systems: A Deep Dive into GraphRAG and LightRAGThe Need for Graph-based RAG Systems](/generative-ai/understanding-graph-based-rag-systems-a-deep-dive-into-graphrag-and-lightrag-daf4f982d7d9?source=post_page---read_next_recirc--75194b4b83d1----3---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>Dec 8<br><br>[182](/generative-ai/understanding-graph-based-rag-systems-a-deep-dive-into-graphrag-and-lightrag-daf4f982d7d9?source=post_page---read_next_recirc--75194b4b83d1----3---------------------6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdaf4f982d7d9&operation=register&redirect=https%3A%2F%2Fgenerativeai.pub%2Funderstanding-graph-based-rag-systems-a-deep-dive-into-graphrag-and-lightrag-daf4f982d7d9&source=---read_next_recirc--75194b4b83d1----3-----------------bookmark_preview----6fdfc3c3_2c66_43ae_8091_bb9ee3c08101-------)<br><br>[See more recommendations](/?source=post_page---read_next_recirc--75194b4b83d1--------------------------------)<br><br>[Help](https://help.medium.com/hc/en-us?source=post_page-----75194b4b83d1--------------------------------)<br><br>[About](/about?autoplay=1&source=post_page-----75194b4b83d1--------------------------------)<br><br>[Careers](/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----75194b4b83d1--------------------------------)<br><br>[Press](pressinquiries@medium.com?source=post_page-----75194b4b83d1--------------------------------)<br><br>[Blog](https://blog.medium.com/?source=post_page-----75194b4b83d1--------------------------------)<br><br>[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----75194b4b83d1--------------------------------)<br><br>[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----75194b4b83d1--------------------------------)<br><br>[Teams](/business?source=post_page-----75194b4b83d1--------------------------------)<br><br>Recaptcha memerlukan pengesahan. <br><br>- <br><br>dilindungi oleh **reCAPTCHA**<br><br>- <br>